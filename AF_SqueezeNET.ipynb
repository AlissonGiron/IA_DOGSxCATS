{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definindo o modelo com arquitetura Squeeze Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, BatchNormalization\n",
    "from keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "bnmomemtum=0.9\n",
    "\n",
    "def fire(x, squeeze, expand):\n",
    "    y  = Conv2D(filters=squeeze, kernel_size=1, activation='relu', padding='same')(x)\n",
    "    y = BatchNormalization(momentum=bnmomemtum)(y)\n",
    "    y1 = Conv2D(filters=expand//2, kernel_size=1, activation='relu', padding='same')(y)\n",
    "    y1 = BatchNormalization(momentum=bnmomemtum)(y1)\n",
    "    y3 = Conv2D(filters=expand//2, kernel_size=3, activation='relu', padding='same')(y)\n",
    "    y3 = BatchNormalization(momentum=bnmomemtum)(y3)\n",
    "    return keras.layers.concatenate([y1, y3])\n",
    "\n",
    "def fire_module(squeeze, expand):\n",
    "    return lambda x: fire(x, squeeze, expand)\n",
    "\n",
    "def get_model(num_classes):\n",
    "    x = keras.layers.Input(shape=(192, 192, 3))\n",
    "\n",
    "    y = Conv2D(kernel_size=3, filters=32, padding='same', use_bias=True, activation='relu')(x)\n",
    "    y = BatchNormalization(momentum=bnmomemtum)(y)\n",
    "    y = fire_module(24, 48)(y)\n",
    "    y = MaxPooling2D(pool_size=2)(y)\n",
    "    y = fire_module(48, 96)(y)\n",
    "    y = MaxPooling2D(pool_size=2)(y)\n",
    "    y = fire_module(64, 128)(y)\n",
    "    y = MaxPooling2D(pool_size=2)(y)\n",
    "    y = fire_module(48, 96)(y)\n",
    "    y = MaxPooling2D(pool_size=2)(y)\n",
    "    y = fire_module(24, 48)(y)\n",
    "    y = GlobalAveragePooling2D()(y)\n",
    "    y = Dense(num_classes, activation='softmax')(y)\n",
    "\n",
    "    return keras.Model(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definindo classe responsável por executar o treinamento com o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib.pyplot import cla\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class AIModule:\n",
    "    def __init__(self, batch_size=32, epochs=5):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.IMG_HEIGHT = 192\n",
    "        self.IMG_WIDTH = 192\n",
    "\n",
    "    def train(self, classes, model_name, train_generator, validation_generator):\n",
    "\n",
    "        # Model\n",
    "        self.model = get_model(len(classes))\n",
    "        self.model.compile(optimizer=SGD(lr=0.001, momentum=0.9),\n",
    "                           loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        img_width, img_height = self.IMG_HEIGHT, self.IMG_WIDTH\n",
    "\n",
    "        epochs = self.epochs\n",
    "        batch_size = self.batch_size\n",
    "      \n",
    "        hist = self.model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=len(train_generator),\n",
    "            epochs=epochs,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=len(validation_generator),\n",
    "            verbose=2)\n",
    "\n",
    "        self.model.save(model_name)\n",
    "\n",
    "        # plot loss\n",
    "        plt.subplot(211)\n",
    "        plt.title('Cross Entropy Loss')\n",
    "        plt.plot(hist.history['loss'], color='blue', label='train')\n",
    "        plt.plot(hist.history['val_loss'], color='orange', label='test')\n",
    "        # plot accuracy\n",
    "        plt.subplot(212)\n",
    "        plt.title('Classification Accuracy')\n",
    "        plt.plot(hist.history['accuracy'], color='blue', label='train')\n",
    "        plt.plot(hist.history['val_accuracy'], color='orange', label='test')\n",
    "\n",
    "        plt.savefig(model_name + '_statistics.png')\n",
    "        plt.close()\n",
    "        \n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7335a579cc0268fba5d34d6f7558f33c187eedb3"
   },
   "source": [
    "# Preparando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "filenames = os.listdir(\"./train/train\")\n",
    "categories = []\n",
    "\n",
    "for filename in filenames:\n",
    "    category = filename.split('.')[0]\n",
    "    if category == 'dog':\n",
    "        categories.append(1)\n",
    "    else:\n",
    "        categories.append(0)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'filename': filenames,\n",
    "    'category': categories\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"category\"] = df[\"category\"].replace({0: 'cat', 1: 'dog'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4eeb7af8dcf02c4ef5ca744c8305c51a2f5cedef"
   },
   "outputs": [],
   "source": [
    "train_df, validate_df = train_test_split(df, test_size=0.20, random_state=42)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "validate_df = validate_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ae3dec0361f0443132d0309d3b883ee80070cf9f"
   },
   "outputs": [],
   "source": [
    "total_train = train_df.shape[0]\n",
    "total_validate = validate_df.shape[0]\n",
    "batch_size=10\n",
    "\n",
    "print('Qtd. Validação: ', total_validate)\n",
    "print('Qtd. Treino: ', total_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff760be9104f7d9492467b8d9d3405011aa77d11"
   },
   "source": [
    "# Utilizando Generators para fazer Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4d1c7818703a8a4bac5c036fdea45972aa9e5e9e"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    rescale=1./255,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df, \n",
    "    \"./train/train/\", \n",
    "    x_col='filename',\n",
    "    y_col='category',\n",
    "    target_size=IMAGE_SIZE,\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "    validate_df, \n",
    "    \"./train/train/\", \n",
    "    x_col='filename',\n",
    "    y_col='category',\n",
    "    target_size=IMAGE_SIZE,\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5cd8df64e794ed17de326b613a9819e7da977a0e"
   },
   "source": [
    "# Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0836a4cc8aa0abf603e0f96573c0c4ff383ad56b"
   },
   "outputs": [],
   "source": [
    "ai_module = AIModule()\n",
    "\n",
    "model = ai_module.train([ 'cat', 'dog' ], 'model.h5', train_generator, validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "764dc66e4b2bc558f3a0f90b80bb802f5b3d45a8"
   },
   "source": [
    "# Prepara os dados para teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c35e70d3e1e4834dbbf840fa0ea08c049bfcd915"
   },
   "outputs": [],
   "source": [
    "test_filenames = os.listdir(\"./test1/test1\")\n",
    "test_df = pd.DataFrame({\n",
    "    'filename': test_filenames\n",
    "})\n",
    "nb_samples = test_df.shape[0]\n",
    "\n",
    "test_gen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_gen.flow_from_dataframe(\n",
    "    test_df, \n",
    "    \"./test1/test1/\", \n",
    "    x_col='filename',\n",
    "    y_col=None,\n",
    "    class_mode=None,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2fa580afca2931ec5ce374e732d8c1789d03f2ed"
   },
   "source": [
    "# Realiza o predict e formata os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4782eb23fa7d003f0e2415d995894017edb2d896"
   },
   "outputs": [],
   "source": [
    "predict = model.predict_generator(test_generator, steps=np.ceil(nb_samples/batch_size))\n",
    "\n",
    "#Verifica qual categoria possui mais possibilidade de ser a correta\n",
    "test_df['category'] = np.argmax(predict, axis=-1)\n",
    "\n",
    "label_map = dict((v,k) for k,v in train_generator.class_indices.items())\n",
    "test_df['category'] = test_df['category'].replace(label_map)\n",
    "test_df['category'] = test_df['category'].replace({ 'dog': 1, 'cat': 0 })\n",
    "\n",
    "sample_test = test_df.head(18)\n",
    "sample_test.head()\n",
    "plt.figure(figsize=(12, 24))\n",
    "for index, row in sample_test.iterrows():\n",
    "    filename = row['filename']\n",
    "    category = row['category']\n",
    "    img = load_img(\"../test1/test1/\"+filename, target_size=IMAGE_SIZE)\n",
    "    plt.subplot(6, 3, index+1)\n",
    "    plt.imshow(img)\n",
    "    plt.xlabel(filename + '(' + \"{}\".format(category) + ')' )\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
